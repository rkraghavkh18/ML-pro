{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1 (default, Oct 28 2018, 08:39:03) [MSC v.1912 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  random \n",
    "import numpy as np \n",
    "import math\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 22 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 24 timesteps\n",
      "Episode finished after 34 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 34 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 23 timesteps\n",
      "Episode finished after 22 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 41 timesteps\n"
     ]
    }
   ],
   "source": [
    "#seting up openai gym environment\n",
    "\n",
    "env= gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation =env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        #print observation\n",
    "        action=env.action_space.sample()\n",
    "        observation,reward,done,info=env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print('Episode finished after {} timesteps'.format(t+1))\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "      \n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 1000\n",
    "n_win_ticks = 195\n",
    "#timestamp is called tick \n",
    "max_env_steps = None\n",
    "\n",
    "gamma = 1.0 #discount factor -future reward\n",
    "epsilon = 1.0 # exploration random walk\n",
    "epsilon_min = 0.01 \n",
    "epsilon_decay  = 0.995\n",
    "alpha = 0.01 #learnning rate\n",
    "alpha_decay = 0.01\n",
    "\n",
    "batch_size = 64\n",
    "monitor =False\n",
    "quiet= False#for controling  print statement\n",
    "\n",
    "memory = deque (maxlen=100000)\n",
    "env= gym.make('CartPole-v0')\n",
    "if max_env_steps is not None: env.max_episode_steps= max_env_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24,input_dim= 4,activation='relu'))\n",
    "\n",
    "model.add(Dense(48,activation='relu'))\n",
    "\n",
    "model.add(Dense(2,activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer = Adam(lr= alpha, decay = alpha_decay) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state,action,reward,next_state,done):\n",
    "    memory.append((state,action,reward,next_state,done))\n",
    "\n",
    "def choose_action(state,epsilon):\n",
    "    return env.action_space.sample() if (np.random.random()<= epsilon) else np.argmax(model.predict(state))\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(epsilon_min , min(epsilon ,1.0-math.log10(t+1)*epsilon_decay))\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return  np.reshape(state,[1,4])\n",
    "\n",
    "def replay(batch_size, epsilon):\n",
    "    x_batch,y_batch=[],[]\n",
    "    minibatch= random.sample(memory,min(len(memory),batch_size))\n",
    "    for state,action,reward,next_state,done in minibatch:\n",
    "        y_target=model.predict(state)\n",
    "        y_target[0][action]=reward if done else reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "        \n",
    "    model.fit(np.array(x_batch),np.array(y_batch),batch_size=len(x_batch),verbose=0)\n",
    "    if(epsilon>epsilon_decay):\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run function \n",
    "def run():\n",
    "    scores=deque(maxlen=100)\n",
    "    \n",
    "    for e in range(n_episode):\n",
    "        state = preprocess_state(env.reset())\n",
    "        done = False\n",
    "        i =0\n",
    "        while not done:\n",
    "            action=choose_action(state,get_epsilon(e))\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            env.render()\n",
    "            next_state=preprocess_state(next_state)\n",
    "            remember(state,action,reward,next_state,done)\n",
    "            state=next_state\n",
    "            i+=1\n",
    "            \n",
    "        scores.append(i)\n",
    "        mean_score=np.mean(scores)\n",
    "        \n",
    "        if mean_score>=n_win_ticks and e>=100:\n",
    "            if not quiet : print('Ran {} episodes Solved after {} trials'.format(e,e-100))    \n",
    "            return e-100 \n",
    "            if (e%20 == 0 and not quiet):\n",
    "                print('[Episode{}] - mean survivial time over last 20 episodes was {} ticks'.format(e,mean_score))\n",
    "            \n",
    "        replay(batch_size,get_epsilon(e))\n",
    "        \n",
    "    if not quiet :print('did not solved after {} episodes'.format(e))\n",
    "    return e\n",
    "            \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 20 episodes was 22.0 ticks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000001D7868106A8>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\91750\\desktop\\dl\\gym\\gym\\envs\\classic_control\\rendering.py\", line 165, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\users\\91750\\desktop\\dl\\gym\\gym\\envs\\classic_control\\rendering.py\", line 83, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\91750\\Anaconda\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 299, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\91750\\Anaconda\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 823, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\91750\\Anaconda\\lib\\_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: (<weakref at 0x000001D789285228; to 'Win32Window' at 0x000001D7FD838550>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 20] - Mean survival time over last 20 episodes was 11.095238095238095 ticks.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Training Parameters\n",
    "n_episodes=1000\n",
    "n_win_ticks=195\n",
    "max_env_steps=None\n",
    "gamma=1.0\n",
    "epsilon=1.0\n",
    "epsilon_min=0.01\n",
    "epsilon_decay=0.995\n",
    "alpha=0.01\n",
    "alpha_decay=0.01\n",
    "batch_size=64\n",
    "monitor=False\n",
    "quiet=False\n",
    "\n",
    "# Environment Parameters\n",
    "memory = deque(maxlen=100000)\n",
    "env = gym.make('CartPole-v0')\n",
    "if max_env_steps is not None: env.max_episode_steps = max_env_steps\n",
    "\n",
    "\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=alpha, decay=alpha_decay))\n",
    "\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(\n",
    "        model.predict(state))\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(epsilon_min, min(epsilon, 1.0 - math.log10((t + 1) * epsilon_decay)))\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return np.reshape(state, [1, 4])\n",
    "\n",
    "def replay(batch_size, epsilon):\n",
    "    x_batch, y_batch = [], []\n",
    "    minibatch = random.sample(\n",
    "        memory, min(len(memory), batch_size))\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target = model.predict(state)\n",
    "        y_target[0][action] = reward if done else reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "\n",
    "    model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "    \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay    \n",
    "        \n",
    "def run():\n",
    "    scores = deque(maxlen=100)\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        state = preprocess_state(env.reset())\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, get_epsilon(e))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            next_state = preprocess_state(next_state)\n",
    "            remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            i += 1\n",
    "\n",
    "        scores.append(i)\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score >= n_win_ticks and e >= 100:\n",
    "            if not quiet: print('Ran {} episodes. Solved after {} trials'.format(e, e - 100))\n",
    "            return e - 100\n",
    "        if e % 20 == 0 and not quiet:\n",
    "            print('[Episode {}] - Mean survival time over last 20 episodes was {} ticks.'.format(e, mean_score))\n",
    "   \n",
    "        replay(batch_size, get_epsilon(e))\n",
    "\n",
    "    if not quiet: print('Did not solve after {} episodes'.format(e))\n",
    "    return e\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
